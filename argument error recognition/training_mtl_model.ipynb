{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2024-08-09T17:17:23.530307Z","iopub.status.busy":"2024-08-09T17:17:23.530020Z","iopub.status.idle":"2024-08-09T17:17:38.626305Z","shell.execute_reply":"2024-08-09T17:17:38.625314Z","shell.execute_reply.started":"2024-08-09T17:17:23.530279Z"},"trusted":true},"outputs":[],"source":["import numpy as np # linear algebra\n","import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n","from tqdm.auto import tqdm\n","import json\n","import copy\n","import random\n","import collections\n","import time\n","import torch\n","from torch import nn, cuda, optim\n","from torch.utils.data import DataLoader\n","import torch.nn.init as init\n","import ast\n","import nltk\n","nltk.download('punkt')\n","from nltk.tokenize import word_tokenize\n","\n","device = 'cuda' if cuda.is_available() else 'cpu'\n","if device == 'cuda':\n","    torch.cuda.empty_cache()\n","print(device)\n","from itertools import islice\n","\n","from transformers import (\n","    BertModel,\n","    BertForTokenClassification,\n","    BertTokenizerFast,\n","    AutoTokenizer,\n","    AutoModelForTokenClassification,\n","    get_linear_schedule_with_warmup\n",")\n","\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import (\n","    f1_score,\n","    accuracy_score,\n","    precision_score,\n","    recall_score,\n","    precision_recall_fscore_support,\n","    classification_report\n",")\n","\n","import os\n","for dirname, _, filenames in os.walk('/kaggle/input'):\n","    for filename in filenames:\n","        print(os.path.join(dirname, filename))\n","        \n","import warnings\n","warnings.filterwarnings('ignore')\n","\n","def generate_random_seed():\n","    return random.randint(1, 1000)\n","\n","def set_random_seed(seed):\n","    random.seed(seed)\n","    np.random.seed(seed)\n","    torch.manual_seed(seed)\n","    if torch.cuda.is_available():\n","        torch.cuda.manual_seed_all(seed)"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2024-08-09T17:17:42.493572Z","iopub.status.busy":"2024-08-09T17:17:42.493098Z","iopub.status.idle":"2024-08-09T17:17:42.519161Z","shell.execute_reply":"2024-08-09T17:17:42.518048Z","shell.execute_reply.started":"2024-08-09T17:17:42.493534Z"},"trusted":true},"outputs":[],"source":["class CustomDataset(torch.utils.data.Dataset):\n","    def __init__(self, encodings, classification_labels, regression_labels):\n","        self.encodings = encodings\n","        self.classification_labels = classification_labels\n","        self.regression_labels = regression_labels\n","\n","    def __getitem__(self, idx):\n","        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n","        item['classification_labels'] = torch.tensor(self.classification_labels[idx])\n","        item['regression_labels'] = torch.tensor(self.regression_labels[idx])\n","        return item\n","\n","    def __len__(self):\n","        return len(self.classification_labels)\n","    \n","class MTLModel(nn.Module):\n","    def __init__(self, num_labels, regression_output_dim = 1):\n","        super(MTLModel, self).__init__()\n","        \n","        self.transformer = BertModel.from_pretrained('bert-base-uncased')\n","        \n","        # Configuración de la capa de clasificación\n","        self.classification_layer = nn.Sequential(\n","            nn.Linear(self.transformer.config.hidden_size, 256),\n","            nn.ReLU(),\n","            nn.Linear(256, num_labels)\n","        )\n","        \n","        # Configuración de la capa de regresión\n","        self.regression_layer = nn.Sequential(\n","            nn.Linear(self.transformer.config.hidden_size, 256),\n","            nn.ReLU(),\n","            nn.Linear(256, regression_output_dim)\n","        )\n","\n","    def forward(self, input_ids, attention_mask, token_type_ids):\n","        x = self.transformer(input_ids, \n","                        token_type_ids = token_type_ids, \n","                        attention_mask = attention_mask,\n","                        output_hidden_states = False)\n","        \n","        pooled_output = x.pooler_output\n","        classification_output = self.classification_layer(pooled_output)\n","        regression_output = self.regression_layer(pooled_output)\n"," \n","        return classification_output, regression_output\n","    \n","def divide_dataset(df, rs, names_classes):\n","    filtered_df = df.copy()    \n","    filtered_df = filtered_df[['PredictedArgument', 'Topic', 'CoarseClass', 'RValue', 'Set']]\n","    X_train = filtered_df[filtered_df['Set'] == 'train']\n","    X_test = filtered_df[filtered_df['Set'] == 'test']\n","    X_train, X_dev = train_test_split(X_train, test_size = 0.10, random_state = rs, stratify = X_train['CoarseClass'])\n","    return X_train, X_dev, X_test\n","\n","    \n","def prepare_input(df, labels_classification, labels_regression, tokenizer, max_len = 128, shuffle = True, bs = 32):\n","    arguments = df['PredictedArgument'].values\n","    topics  = df['Topic'].values\n","    arguments = [str(arg) for arg in arguments]\n","    topics = [str(tp) for tp in topics]\n","    \n","    encodings = tokenizer(list(arguments), list(topics), \n","                          truncation=True, \n","                          padding='max_length', \n","                          max_length=max_len)\n","    dst = CustomDataset(encodings, labels_classification, labels_regression)\n","    dataloader = DataLoader(dst, batch_size = bs, shuffle = shuffle)\n","    return dataloader\n","\n","def get_batch_predictions(model, batch, model_name = 'bert', classification_loss_fn = None, regression_loss_fn = None):\n","    # mini-batch predictions\n","    input_ids = batch['input_ids'].to(device)\n","    attention_mask = batch['attention_mask'].to(device)\n","    input_type_ids = None\n","    if model_name == 'bert':\n","        input_type_ids = batch['token_type_ids'].to(device)\n","    classification_labels = batch['classification_labels'].type(torch.LongTensor).to(device)  # BS x1\n","    regression_labels = batch['regression_labels'].type(torch.FloatTensor).to(device)\n","            \n","    # get predictions for the mini-batch\n","    classification_outputs, regression_outputs = model(input_ids, attention_mask, input_type_ids) # outputs BS x 2\n","            \n","    # calculate mini-batch loss and accuracy\n","    loss = None\n","    if classification_loss_fn is not None:\n","        classification_loss = classification_loss_fn(classification_outputs, classification_labels)\n","        regression_loss = regression_loss_fn(regression_outputs, regression_labels)\n","        \n","        loss = classification_loss + regression_loss\n","    \n","    return (classification_outputs, classification_labels), (regression_outputs, regression_labels) , loss\n","\n","# save model to disc\n","def save_model_state(model_state, PATH):\n","    torch.save(model_state, PATH)\n","    \n","# load and returns a previously saved model\n","def load_model(PATH):\n","    model = MTLModel(NUM_CLASSES)\n","    model.load_state_dict(torch.load(PATH))\n","    return model\n","\n","def transform_label(x, name_classes):\n","    return name_classes.index(x)\n","\n","def save_training_information(info, exp):\n","    cols = info[0]\n","    pd.DataFrame(info[1:], columns = cols).to_csv(f'results_train_{exp}.csv', index = False)"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2024-08-09T17:17:44.986807Z","iopub.status.busy":"2024-08-09T17:17:44.985963Z","iopub.status.idle":"2024-08-09T17:17:45.009188Z","shell.execute_reply":"2024-08-09T17:17:45.008225Z","shell.execute_reply.started":"2024-08-09T17:17:44.986771Z"},"trusted":true},"outputs":[],"source":["def finetune_model(model, train_loader, validation_loader, class_loss_fn, regr_loss_fn, optim, epochs, n_samples, n_eval_samples, model_checkpoint):\n","    best_model_state = copy.deepcopy(model.state_dict())\n","    best_loss = float('inf')\n","    progress_bar = tqdm(range(len(train_loader) * epochs))\n","    output_info = []\n","    for epoch in range(1, epochs + 1):\n","        \n","        print(\"Epoch: {}/{}\".format(epoch, epochs))\n","        start_time = time.time()\n","        \n","        ## TRAINING LOOP\n","        start_time = time.time()\n","        epoch_info = [epoch]\n","        train_loss, train_hits = 0, 0\n","        model.train()\n","        \n","        for i, batch in enumerate(train_loader, 0):\n","            optim.zero_grad()\n","            \n","            class_info, regr_info, loss = get_batch_predictions(model, batch, model_checkpoint.split('-')[0], classification_loss_fn = class_loss_fn, regression_loss_fn = regr_loss_fn)\n","            outputs, labels = class_info\n","            train_loss += loss.item()\n","            train_hits += (torch.argmax(outputs, dim = 1) == labels).sum().item()\n","            \n","            # Optimization step\n","            loss.backward()\n","            \n","            # gradient clipping\n","            torch.nn.utils.clip_grad_norm_(parameters=model.parameters(), max_norm=1.0)\n","            \n","            # update learning rates using scheduler\n","            optim.step()\n","            \n","            progress_bar.update(1)\n","          \n","        # calculate average loss and accuracy\n","        avg_train_loss = round(float(train_loss / n_samples), 4)\n","        train_acc = round(float(train_hits / n_samples), 3)\n","            \n","        ## VALIDATION LOOP\n","        model.eval()\n","        eval_loss = 0\n","        eval_predictions, eval_labels = np.array([], int), np.array([], int)\n","        \n","        with torch.no_grad():\n","            for batch in validation_loader:\n","                class_info, regr_info, loss = get_batch_predictions(model, batch, model_checkpoint.split('-')[0], classification_loss_fn = class_loss_fn, regression_loss_fn = regr_loss_fn)\n","                outputs, labels = class_info\n","                eval_loss += loss.item()\n","                \n","                eval_predictions = np.concatenate((eval_predictions, torch.argmax(outputs, dim = 1).int().cpu().numpy()), axis = 0)\n","                eval_labels = np.concatenate((eval_labels, labels.int().cpu().numpy()), axis = 0)\n","                \n","        # calculate average loss, accuracy and macro f1 score\n","        avg_eval_loss = round(float(eval_loss / n_eval_samples), 4)\n","        eval_acc = accuracy_score(eval_labels, eval_predictions)\n","        eval_macro_f1 = f1_score(eval_labels, eval_predictions, average = 'macro')\n","        \n","        ## PRINT INFO\n","        print(f\"Training   ==> Accuracy: {train_acc} | Loss: {avg_train_loss}\")\n","        print(f\"Validation ==> Accuracy: {eval_acc}  | Loss: {avg_eval_loss}  | Macro-F1: {eval_macro_f1}\")\n","        \n","        ## SAVE BEST MODEL\n","        ## Save model state when validation loss decreases\n","        if eval_loss < best_loss:\n","            best_model_state = copy.deepcopy(model.state_dict())\n","            best_loss = eval_loss\n","            print(\"Validation loss has decreased... saving model at epoch {}\".format(epoch))\n","            \n","        epoch_info += [avg_train_loss, train_acc, avg_eval_loss, eval_acc, eval_macro_f1, n_samples, n_eval_samples]\n","        output_info.append(epoch_info)\n","    \n","    output_info.insert(0, ['epoch', 'avg_train_loss', 'train_acc', 'avg_eval_loss', 'eval_acc', 'eval_macro_f1', 'train_samples', 'eval_samples'])\n","    return best_model_state, best_loss, output_info\n","\n","def test_model(path, dataloader, model_checkpoint, classes = None):\n","    model = load_model(path).to(device)\n","    model.eval()\n","    \n","    predictions, test_labels = np.array([], int), np.array([], int)\n","    predictions_regr = np.array([])\n","    test_labels_regr = np.array([])\n","    logits = np.zeros((0, NUM_CLASSES), dtype=np.float32)\n","    \n","    with torch.no_grad():\n","        for batch in dataloader:\n","            class_info, regr_info, loss = get_batch_predictions(model, batch, model_checkpoint.split('-')[0])\n","            outputs, labels = class_info\n","            outputs_regr, labels_regr = regr_info\n","                        \n","            logits = np.concatenate((logits, outputs.cpu().numpy()), axis = 0)\n","            predictions = np.concatenate((predictions, torch.argmax(outputs, dim = 1).int().cpu().numpy()), axis = 0)\n","            predictions_regr = np.concatenate((predictions_regr, outputs_regr.cpu().numpy().flatten()), axis = 0)\n","            test_labels = np.concatenate((test_labels, labels.int().cpu().numpy()), axis = 0)\n","            test_labels_regr = np.concatenate((test_labels_regr, labels_regr.cpu().numpy()), axis = 0)\n","    \n","    return predictions, logits, predictions_regr"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-08-09T17:17:49.104079Z","iopub.status.busy":"2024-08-09T17:17:49.103605Z","iopub.status.idle":"2024-08-09T21:29:45.260275Z","shell.execute_reply":"2024-08-09T21:29:45.259285Z","shell.execute_reply.started":"2024-08-09T17:17:49.104044Z"},"trusted":true},"outputs":[],"source":["MAX_LENGTH = 128\n","task = 'multi'\n","df_name = 'generated_ukp'\n","df = pd.read_csv(f\"/kaggle/input/errortypesclassification/{df_name}.csv\")\n","\n","names_classes = ['PM', 'DISP', 'SP', 'MG', 'MU'] if task == 'multi' else ['No-PM', 'PM']\n","NUM_CLASSES = len(names_classes)\n","\n","model_checkpoint = 'bert-base-uncased'\n","tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n","\n","epochs = 10 if task == 'multi' else 5 # number of epochs for each training run \n","experiments = 10 # number of models that will be trained and tested.\n","\n","seeds_df = pd.DataFrame(columns=['r_seed', 'indices', 'test_size'])\n","\n","best_model_exp = 0\n","best_model_eval_loss = float('inf')\n","\n","for exp in range(experiments):\n","    \n","    print(f\"Running experiment number {exp+1}\")\n","    \n","    # setup new model, optimizer, loss function and scheduler before training\n","    # different random seeds are used in each experiment\n","    r_seed = generate_random_seed()\n","    set_random_seed(r_seed)\n","    \n","    # MODEL\n","    model = MTLModel(NUM_CLASSES)\n","    model.to(device)\n","    \n","    # LOSS FUNCTION\n","    classification_loss_fn = nn.CrossEntropyLoss()\n","    regression_loss_fn = nn.MSELoss()\n","\n","    optim = torch.optim.AdamW(model.parameters(), lr = 1e-5, eps = 1e-8)\n","    \n","    # TRAINING DATA\n","    X_train, X_dev, X_test = divide_dataset(df, r_seed, names_classes)\n","    \n","    current_df = pd.DataFrame({'r_seed': [r_seed], 'indices': [X_test.index.values.tolist()], 'test_size': len(X_test)})\n","    seeds_df = pd.concat([seeds_df, current_df], ignore_index=True)\n","    \n","    y_train_class, y_dev_class, y_test_class = X_train.pop(\"CoarseClass\").values, X_dev.pop(\"CoarseClass\").values, X_test.pop(\"CoarseClass\").values\n","    y_train_regr, y_dev_regr, y_test_regr = X_train.pop(\"RValue\").values, X_dev.pop(\"RValue\").values, X_test.pop(\"RValue\").values\n","    train_samples, eval_samples, test_samples = len(y_train_class), len(y_dev_class), len(y_test_class)\n","    print(train_samples, eval_samples, test_samples)\n","    \n","    if task == 'multi':\n","        y_train_class = [transform_label(x, names_classes) for x in y_train_class]\n","        y_dev_class = [transform_label(x, names_classes) for x in y_dev_class]\n","        y_test_class = [transform_label(x, names_classes) for x in y_test_class]\n","    elif task == 'binary':\n","        y_train_class = [1 if x == 'PM' else 0 for x in y_train_class]\n","        y_dev_class = [1 if x == 'PM' else 0 for x in y_dev_class]\n","        y_test_class = [1 if x == 'PM' else 0 for x in y_test_class]\n","    \n","    \n","    # TRAINING DATA\n","    train_loader = prepare_input(X_train, y_train_class, y_train_regr, tokenizer, bs = 32, shuffle = True)\n","    validation_loader = prepare_input(X_dev, y_dev_class, y_dev_regr, tokenizer, bs = 32, shuffle = True)\n","    test_loader = prepare_input(X_test, y_test_class, y_test_regr, tokenizer, bs = 32, shuffle = False)\n","    \n","    best_model, eval_error, train_info = finetune_model(model, train_loader, validation_loader, \n","                                classification_loss_fn, regression_loss_fn, optim, epochs, \n","                                train_samples, eval_samples, model_checkpoint)\n","        \n","    best_model_name = f\"trained_model_exp_{exp}.pt\"\n","    save_model_state(best_model, best_model_name)\n","    save_training_information(train_info, exp)\n","    \n","    test_predictions, test_logits, test_regr_predictions = test_model(best_model_name, test_loader, model_checkpoint, classes = names_classes)\n","    predictions_df = pd.DataFrame()\n","    predictions_df['true_label'] = y_test_class\n","    predictions_df['prediction'] = test_predictions\n","    predictions_df['logits_prediction'] = test_logits.tolist()\n","    predictions_df['regresion_label'] = y_test_regr\n","    predictions_df['regresion_prediction'] = test_regr_predictions\n","    predictions_df.to_csv(f\"test_results_exp_{exp}.csv\", index = False)\n","    \n","    if eval_error < best_model_eval_loss:\n","        best_model_eval_loss = eval_error\n","        best_model_exp = exp\n","    \n","\n","    print(f\"End of experiment number {exp+1}\")\n","    print()\n","    \n","seeds_df.to_csv(f\"seeds_{exp}.csv\", index = False)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-08-09T21:29:45.262069Z","iopub.status.busy":"2024-08-09T21:29:45.261761Z","iopub.status.idle":"2024-08-09T21:30:10.117783Z","shell.execute_reply":"2024-08-09T21:30:10.116805Z","shell.execute_reply.started":"2024-08-09T21:29:45.262044Z"},"trusted":true},"outputs":[],"source":["import os\n","import zipfile\n","from IPython.display import FileLink\n","\n","model_to_save_name = f'trained_model_exp_{best_model_exp}'\n","print(model_to_save_name)\n","\n","def zip_files(folder_path, zip_name):\n","    # Crear un archivo ZIP\n","    with zipfile.ZipFile(zip_name, 'w', zipfile.ZIP_DEFLATED) as zipf:\n","        # Recorrer todos los archivos en la carpeta\n","        for foldername, subfolders, filenames in os.walk(folder_path):\n","            \n","            for filename in filenames:\n","                if ((filename.endswith(('.txt', '.csv'))) and (not filename.startswith(\"sampling\"))) or filename == f\"{model_to_save_name}.pt\":\n","                    # Ruta completa del archivo\n","                    file_path = os.path.join(foldername, filename)\n","                    # Agregar el archivo al archivo ZIP\n","                    zipf.write(file_path, os.path.relpath(file_path, folder_path))\n","\n","# Llamar a la función para comprimir los archivos\n","folder_path = '/kaggle/working/'\n","zip_name = 'test.zip'\n","zip_files(folder_path, zip_name)\n","\n","FileLink(r'test.zip')"]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"datasetId":3919204,"sourceId":9033560,"sourceType":"datasetVersion"}],"dockerImageVersionId":30559,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"}},"nbformat":4,"nbformat_minor":4}

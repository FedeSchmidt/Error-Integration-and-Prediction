{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2024-08-31T18:21:55.009399Z","iopub.status.busy":"2024-08-31T18:21:55.009019Z","iopub.status.idle":"2024-08-31T18:22:00.407312Z","shell.execute_reply":"2024-08-31T18:22:00.406295Z","shell.execute_reply.started":"2024-08-31T18:21:55.009368Z"},"trusted":true},"outputs":[],"source":["\n","import numpy as np # linear algebra\n","import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n","from tqdm.auto import tqdm\n","import json\n","import collections\n","import copy\n","import random\n","import time\n","import torch\n","from torch import nn, cuda, optim\n","from torch.utils.data import DataLoader\n","device = 'cuda' if cuda.is_available() else 'cpu'\n","if device == 'cuda':\n","    torch.cuda.empty_cache()\n","print(device)\n","\n","from transformers import (\n","    BertModel,\n","    BertForTokenClassification,\n","    BertTokenizerFast,\n","    AutoTokenizer,\n","    AutoModelForTokenClassification,\n","    get_linear_schedule_with_warmup\n",")\n","\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import (\n","    f1_score,\n","    accuracy_score,\n","    precision_score,\n","    recall_score,\n","    precision_recall_fscore_support,\n","    classification_report\n",")\n","\n","import os\n","for dirname, _, filenames in os.walk('/kaggle/input'):\n","    for filename in filenames:\n","        print(os.path.join(dirname, filename))\n","        \n","import warnings\n","warnings.filterwarnings('ignore')\n","\n","def generate_random_seed():\n","    return random.randint(1, 1000)\n","\n","def set_random_seed(seed):\n","    random.seed(seed)\n","    np.random.seed(seed)\n","    torch.manual_seed(seed)\n","    if torch.cuda.is_available():\n","        torch.cuda.manual_seed_all(seed)\n","\n","# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n","# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2024-08-31T18:22:09.259746Z","iopub.status.busy":"2024-08-31T18:22:09.259208Z","iopub.status.idle":"2024-08-31T18:22:09.267668Z","shell.execute_reply":"2024-08-31T18:22:09.266696Z","shell.execute_reply.started":"2024-08-31T18:22:09.259714Z"},"trusted":true},"outputs":[],"source":["class Config:\n","    def __init__(self, runs, epochs, batch_size, \n","                 num_labels = 3, label_list = ['O', 'B', 'I'], model_checkpoint = \"bert-base-uncased\", max_len = 128, error_type = 'simple', use_crf = False):\n","        self.num_labels = num_labels\n","        self.label_list = label_list\n","        self.model_checkpoint = model_checkpoint\n","        self.tokenizer = AutoTokenizer.from_pretrained(self.model_checkpoint)\n","        self.runs = runs\n","        self.epochs = epochs\n","        self.batch_size = batch_size\n","        self.max_len = max_len\n","        self.error_type = error_type\n","        self.use_crf = use_crf"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2024-08-31T18:22:11.029834Z","iopub.status.busy":"2024-08-31T18:22:11.029502Z","iopub.status.idle":"2024-08-31T18:22:11.066006Z","shell.execute_reply":"2024-08-31T18:22:11.065041Z","shell.execute_reply.started":"2024-08-31T18:22:11.029807Z"},"trusted":true},"outputs":[],"source":["\"\"\" Tokenize examples in batch\n","Since the tokenizer may divide each token into two or more subtokens, we must align the new tokens with the original labels.\n","New subtokens must have the same label than their parent token\n","Labels may be 0, 1 or 2 for O, B and I labels, respectively, and -100 for complementary tokens, such PAD, SEP, CLS tokens.\n","Loss functions will ignore labels with value -100, so the loss only considers mistakes at the positions of real input (sub)tokens.\n","\"\"\"\n","def tokenize_and_align_labels(txts, lbls, config):\n","    tokenizer, max_len, mapping = config.tokenizer, config.max_len, config.label_list\n","\n","    tokenized_inputs = tokenizer(txts, is_split_into_words=True,\n","                                 max_length = max_len, \n","                                 padding = 'max_length', \n","                                 truncation=True,\n","                                 return_tensors = 'pt')\n","\n","    labels = []\n","    for i, label in enumerate(lbls):\n","        word_ids = tokenized_inputs.word_ids(batch_index=i)\n","        previous_word_idx = None\n","        previous_label = None\n","        label_ids = []\n","        for word_idx in word_ids:\n","            # Special tokens have a word id that is None. We set the label to -100 so they are automatically\n","            # ignored in the loss function.            \n","            if word_idx is None:\n","                label_ids.append(-100)\n","            # We set the label for the first token of each word.\n","            elif word_idx != previous_word_idx:\n","                label_ids.append(mapping.index(label[word_idx]))\n","                previous_label = label[word_idx]\n","            # For the other tokens in a word, we set the label to the current label.\n","            else:\n","                new_label = 'O' if previous_label == 'O' else 'I'+previous_label[1:]\n","                label_ids.append(mapping.index(new_label))\n","                previous_label = new_label\n","                \n","            previous_word_idx = word_idx\n","\n","        labels.append(label_ids)\n","\n","    return tokenized_inputs, labels\n","\n","\"\"\"\n","Return text tokens from the tokenizer given the numeric input ids\n","\"\"\"\n","def get_tokens_from_ids(input_ids):\n","\n","    return [tokenizer.convert_ids_to_tokens(tl) for tl in input_ids]\n","\n","\"\"\"\n","Remove part of predicted sequences corresponding to padding tokens.\n","\"\"\"\n","def remove_padding_from_predictions(predictions, batch_attention_mask):\n","    valid_predictions_list = []\n","    for instance_preds, att_mask in zip(predictions, batch_attention_mask):\n","        valid = [pred for pred, mask in zip(instance_preds, att_mask) if mask == 1]\n","        valid_predictions_list.append(valid[1:-1])\n","        \n","    return valid_predictions_list\n","\n","def remove_padding_and_get_tokens(batch_ids, batch_attention_mask):\n","    valid_ids_list = []\n","    for instances_ids, att_mask in zip(batch_ids, batch_attention_mask):\n","        valid = [ids for ids, mask in zip(instances_ids, att_mask) if mask == 1]\n","        valid_ids_list.append(valid[1:-1])\n","    \n","    valid_tokens = get_tokens_from_ids(valid_ids_list)\n","    return valid_tokens\n","\n","\"\"\"\n","Maps sequences of integer to sequences of BIO tags\n","\"\"\"\n","def integer_to_bio(labels, mapping):\n","    return [[mapping[int(x)] for x in l] for l in labels]\n","\n","\"\"\"\n","Transforms list of predicted sequences to a flat list of labels.\n","\"\"\"\n","def flatten_predictions(labels):\n","    return [j for sub in labels for j in sub]\n","\n","\"\"\"\n","Generates txt file with tokens, labels and predictions. Estilo FLAiR NLP.\n","\"\"\"\n","def save_predictions(tokens, labels, predictions, file_path):\n","    with open(file_path, 'w', encoding = 'utf-8') as nf:\n","\n","        for tks, lbs, prds in zip(tokens, labels, predictions):\n","            for tk, lb, pr in zip(tks, lbs, prds):\n","                nf.write(f\"{tk} {lb} {pr}\\n\")\n","\n","            nf.write(f\"\\n\") \n","\n","\"\"\"\n","loading data\n","\"\"\"\n","def load_data(df, config):\n","    \n","    train_seq_df = df.loc[df['set'] == 'train']\n","    if 'dev' in df['set'].values:\n","        val_seq_df = df.loc[df['set'] == 'dev']\n","    else:\n","        train_seq_df, val_seq_df = train_test_split(train_seq_df, test_size = 0.1, random_state = 2023)\n","    test_seq_df = df.loc[df['set'] == 'test']\n","\n","    train_dataset, val_dataset, test_dataset = SequenceLabelingDataset(train_seq_df, config), \\\n","                                            SequenceLabelingDataset(val_seq_df, config), \\\n","                                            SequenceLabelingDataset(test_seq_df, config)\n","    \n","    batch_size = config.batch_size\n","    train_loader, val_loader, test_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True), \\\n","                                            DataLoader(val_dataset, batch_size=batch_size), \\\n","                                            DataLoader(test_dataset, batch_size=batch_size)\n","        \n","    return train_loader, val_loader, test_loader            \n","\n","def clean_batch_elements(batch_input_ids, batch_labels, batch_predictions, tokenizer):\n","    eval_tokens = []\n","    eval_labels = []\n","    eval_predictions = []\n","    for i in range(len(batch_input_ids)):\n","        tokens = tokenizer.convert_ids_to_tokens(batch_input_ids[i].tolist())\n","        labels = batch_labels[i]\n","        preds = batch_predictions[i]\n","        filtered_tokens, filtered_labels, filtered_preds = [], [], []\n","        for tk, lb, pr in zip(tokens, labels, preds):\n","            if tk not in tokenizer.all_special_tokens:\n","#                 tk = tk.lstrip('Ä ')\n","                if tk != '':\n","                    filtered_tokens.append(tk)\n","                    filtered_labels.append(lb)\n","                    filtered_preds.append(pr)\n","                \n","        eval_tokens.append(filtered_tokens)\n","        eval_labels.append(filtered_labels)\n","        eval_predictions.append(filtered_preds)\n","        \n","    return eval_tokens, eval_labels, eval_predictions\n","\n","\n","\n","\n","\"\"\"\n","Dataset class for sequence labeling\n","\"\"\"\n","class SequenceLabelingDataset(torch.utils.data.Dataset):\n","    \n","    def __init__(self, df, config):\n","        lb = [x.split() for x in df.labels.values.tolist()]\n","        txt = [i.split() for i in df.tokens.values.tolist()]\n","        self.encodings, self.labels = tokenize_and_align_labels(txt, lb, config)\n","\n","    def __getitem__(self, idx):\n","        item = {key: val[idx].clone().detach() for key, val in self.encodings.items()}\n","        item['labels'] = torch.tensor(self.labels[idx])\n","        return item\n","\n","    def __len__(self):\n","        return len(self.labels)\n","    \n","    \n","\"\"\"\n","Model for sequence labeling\n","\"\"\"\n","class SimpleTagger(nn.Module):\n","    def __init__(self, config):\n","        super(SimpleTagger, self).__init__()\n","        self.model_checkpoint = config.model_checkpoint\n","        self.num_labels = config.num_labels\n","        self.transf = AutoModelForTokenClassification.from_pretrained(self.model_checkpoint, num_labels = self.num_labels)\n","    \n","    def forward(self, input_ids, attention_mask, token_type_ids, labels = None):\n","        \n","        if labels is not None: # training\n","            x = self.transf(input_ids = input_ids, \n","                                    token_type_ids = token_type_ids, \n","                                    attention_mask = attention_mask, \n","                                    labels = labels)\n","            return x.logits\n","        else: # inference\n","            x = self.transf(input_ids = input_ids, \n","                                    token_type_ids = token_type_ids, \n","                                    attention_mask = attention_mask, \n","                                    labels = labels)\n","            \n","            return np.argmax(x.logits.detach().cpu().numpy(), axis = 2).tolist()"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2024-08-31T18:22:15.876794Z","iopub.status.busy":"2024-08-31T18:22:15.876436Z","iopub.status.idle":"2024-08-31T18:22:15.920583Z","shell.execute_reply":"2024-08-31T18:22:15.919567Z","shell.execute_reply.started":"2024-08-31T18:22:15.876767Z"},"trusted":true},"outputs":[],"source":["class CustomLoss():\n","    def __init__(self, base_loss_fn, init_weights, config):\n","        super(CustomLoss, self).__init__()\n","        self.base_loss_fn = base_loss_fn\n","        self.config = config\n","        self.initial_error_weights = init_weights # for static weighting\n","        self.lambdas = init_weights               # for dynamic weighting\n","        self.current_epoch_errors = np.zeros(5)   # num errors per category in current training epoch\n","        self.past_epoch_errors = np.zeros(5)      # num errors per category in last training epoch\n","        self.acumulated_error_epoch = np.zeros(6)\n","        self.matrix_errors = np.zeros((config.epochs, 5))\n","        self.type_weighting = 1\n","        \n","        \n","    def get_category_weights(self):\n","        return self.lambdas\n","        \n","    def restart_error_counter(self):\n","        self.acumulated_error_epoch = np.zeros(6)\n","        \n","    def get_instances_from_batch(self, batch_predictions, batch_labels):\n","        \n","        batch_predictions = batch_predictions.detach().cpu().numpy()\n","        batch_labels = batch_labels.detach().cpu().numpy()\n","        \n","        instances = []\n","        for i in range(len(batch_predictions)):\n","            labels = batch_labels[i]\n","            preds = batch_predictions[i]\n","            \n","            filtered_labels, filtered_preds = [], []\n","            for l, p in zip(labels, preds):\n","                if l != -100:\n","                    filtered_labels.append(l)\n","                    filtered_preds.append(p)\n","                    \n","            instances.append((filtered_labels, filtered_preds))\n","            \n","        return instances\n","    \n","    def get_arguments_indices(self, sequence):\n","        indices = []\n","        start_index = None\n","        for i, label in enumerate(sequence):\n","            if label == 1:\n","\n","                if start_index is None:\n","                    start_index = i\n","                else:\n","                    indices.append((start_index, i))\n","                    start_index = i\n","\n","            elif (label == 0) and (start_index is not None):\n","                indices.append((start_index, i))\n","                start_index = None\n","\n","        if start_index is not None:\n","            indices.append((start_index, len(sequence)))\n","\n","        return indices\n","    \n","    def compute_r_matrix(self, gold_indices, predicted_indices):\n","\n","        def get_R(gold_argument, predicted_argument):\n","            (gold_start, gold_end) = gold_argument\n","            (pred_start, pred_end) = predicted_argument\n","\n","            intersection_start = max(gold_start, pred_start)\n","            intersection_end = min(gold_end, pred_end)\n","\n","            len_intersection_interval = (intersection_end - intersection_start) if intersection_start <= intersection_end else 0\n","            len_longer_span = max(gold_end - gold_start, pred_end - pred_start)\n","            if len_longer_span > 0:\n","                return round((len_intersection_interval / len_longer_span), 3)\n","            else:\n","                return 0\n","\n","        R_matrix = np.zeros((len(gold_indices), len(predicted_indices)), dtype=float)\n","\n","        for i, gold_argument in enumerate(gold_indices):\n","            for j, predicted_argument in enumerate(predicted_indices):\n","                R_matrix[i][j] = get_R(gold_argument, predicted_argument)\n","\n","        return R_matrix\n","    \n","    def categorization_counter(self, matrix, tau, omega):\n","\n","        def is_split(matrix, column):\n","            gold_row_index = np.where(column > 0)[0]\n","            gold_row = matrix[gold_row_index, :]\n","            positive_values = gold_row[gold_row > 0].tolist()\n","            if len(positive_values) > 1:\n","                sum_values = sum(positive_values)\n","                greater_than_omega = [x >= omega for x in positive_values]\n","                if (sum_values >= tau) and (all(greater_than_omega)):\n","                    return True\n","\n","            return False\n","\n","        gold_arguments_names = [f'G{i}' for i in range(matrix.shape[0])]\n","        predicted_arguments_names = [f'P{i}' for i in range(matrix.shape[1])]\n","\n","        categorization = { 'PM': 0, 'DISP': 0, 'SP': 0, 'MG': 0, 'MU': 0, 'UNR': 0 }\n","\n","        for i, pred_arg in enumerate(predicted_arguments_names):\n","            column = matrix[:, i]\n","            positive_values = column[column > 0].tolist()\n","            if len(positive_values) == 0:\n","                categorization['MU'] += 1\n","            elif len(positive_values) == 1:\n","                if positive_values[0] == 1:\n","                    categorization['PM'] += 1\n","                else:\n","                    if is_split(matrix, column):\n","                        categorization['SP'] += 1\n","                    else:\n","                        categorization['DISP'] += 1\n","\n","            elif len(positive_values) > 1:\n","                sum_values = sum(positive_values)\n","                greater_than_omega = [x >= omega for x in positive_values]\n","\n","                if (sum_values >= tau) and (all(greater_than_omega)):\n","                    categorization['MG'] += 1\n","                else:\n","                    categorization['DISP'] += 1\n","\n","        for i, gold_arg in enumerate(gold_arguments_names):\n","            row = matrix[i, :]\n","            positive_values = row[row > 0].tolist()\n","            if len(positive_values) == 0:\n","                categorization['UNR'] += 1\n","\n","        return categorization\n","    \n","    \n","    def update_category_weights(self, current_epoch):\n","        \n","        self.matrix_errors[current_epoch, :] = self.acumulated_error_epoch[1:]\n","        \n","        if self.type_weighting == 1:\n","            \n","            if current_epoch > 0:\n","                current_row = self.matrix_errors[current_epoch, :]\n","                past_row = self.matrix_errors[current_epoch - 1, :]\n","                ratios = [c / p if p != 0 else c for c, p in zip(current_row, past_row)]\n","                exp_sum = sum(np.exp(wx) for wx in ratios)\n","                \n","                for i, wx in enumerate(ratios):\n","                    self.lambdas[i] = np.exp(wx) / exp_sum\n","                \n","            else:\n","                self.lambdas = [1, 1, 1, 1, 1]\n","            \n","          \n","        elif self.type_weighting == 2:\n","            \n","            if current_epoch > 0:\n","                current_row = self.matrix_errors[current_epoch, :]\n","                start_index = max(0, current_epoch - 3)\n","                past_row = np.mean(self.matrix_errors[start_index:current_epoch, :], axis=0)\n","                \n","                ratios = [c / p if p != 0 else c for c, p in zip(current_row, past_row)]\n","                exp_sum = sum(np.exp(wx) for wx in ratios)\n","                \n","                for i, wx in enumerate(ratios):\n","                    self.lambdas[i] = np.exp(wx) / exp_sum\n","            else:\n","                self.lambdas = [1, 1, 1, 1, 1]\n","            \n","          \n","        elif self.type_weighting == 3:\n","            current_row = self.matrix_errors[current_epoch, :]\n","            current_row_sum = np.sum(current_row)\n","            \n","            for i, wx in enumerate(current_row):\n","                self.lambdas[i] = wx / current_row_sum\n","        \n","        else:\n","            raise Exception\n","\n","    def compute_loss(self, batch_logits, batch_labels, validation = False):\n","        \n","        # Auxiliary loss functions\n","        def simple_loss(ce_loss):\n","            return ce_loss\n","        \n","        def pm_guided_loss(ce_loss, num_pm, total_gold_arguments):\n","            pm_loss = 1 - (num_pm / total_gold_arguments)\n","            pm_loss.requires_grad_()\n","            return pm_loss\n","        \n","        def ce_pm_loss(ce_loss, num_errors_tensor, total_gold_arguments):\n","            pm_loss = 1 - (num_errors_tensor[0] / total_gold_arguments)\n","            return ce_loss + 1*pm_loss\n","        \n","        def ce_sum_errors_loss(ce_loss, num_errors_tensor):\n","            total_errors = torch.sum(num_errors_tensor)\n","            total_errors.requires_grad_()\n","            return ce_loss + 0.01*total_errors\n","        \n","        def ce_sum_weighted_errors(ce_loss, num_errors_tensor):\n","            weighted_sum = torch.sum(torch.stack([p*x for p,x in zip(self.initial_error_weights, num_errors_tensor)]))\n","            total_loss = ce_loss + weighted_sum\n","            return total_loss\n","        \n","        def ce_sum_weighted_errors_dyn(ce_loss, num_errors_tensor):\n","            weighted_sum = torch.sum(torch.stack([p*x for p,x in zip(self.lambdas, num_errors_tensor)]))\n","            total_loss = ce_loss + weighted_sum\n","            return total_loss\n","        \n","        ce_loss = self.base_loss_fn(batch_logits.view(-1, 3), batch_labels.view(-1).long())\n","        \n","        batch_predictions = torch.argmax(batch_logits, dim=2)\n","        \n","        instances = self.get_instances_from_batch(batch_predictions, batch_labels)\n","                \n","        num_items_per_category = collections.Counter( { 'PM': 0, 'DISP': 0, 'SP': 0, 'MG': 0, 'MU': 0, 'UNR': 0 })\n","        \n","        total_gold_arguments =  0\n","        \n","        for (labels, predictions) in instances:\n","            ground_truth_indices = self.get_arguments_indices(labels)\n","            predicted_args_indices = self.get_arguments_indices(predictions)\n","            \n","            R_matrix = self.compute_r_matrix(ground_truth_indices, predicted_args_indices)\n","            cat_counter = self.categorization_counter(R_matrix, 0.7, 0.35)\n","            \n","            num_items_per_category.update(cat_counter)\n","                    \n","            total_gold_arguments += len(ground_truth_indices)\n","            \n","        # Compute loss based on the difference between predicted and ground truth arguments\n","        num_errors_list = [num_items_per_category[x] for x in ['PM', 'DISP', 'SP', 'MG', 'MU', 'UNR']]\n","        num_errors_tensor = torch.tensor(num_errors_list, dtype=torch.float32)\n","        \n","        total_gold_arguments_tensor = torch.tensor([total_gold_arguments])\n","        \n","        if not validation:\n","            self.acumulated_error_epoch += np.array(num_errors_tensor)\n","        \n","        # Compute loss\n","        if self.config.error_type == 'simple':\n","            total_loss = simple_loss(ce_loss)\n","        elif self.config.error_type == 'pm-guided':\n","            total_loss = pm_guided_loss(ce_loss, num_errors_tensor[0], total_gold_arguments)\n","        elif self.config.error_type == 'ce-sum-errors':\n","            total_loss = ce_sum_errors_loss(ce_loss, num_errors_tensor[1:])\n","        elif self.config.error_type == 'ce-sum-weighted-errors':\n","            total_loss = ce_sum_weighted_errors(ce_loss, num_errors_tensor[1:])\n","        elif self.config.error_type == 'ce-sum-weighted-errors-dynamic':\n","            total_loss = ce_sum_weighted_errors_dyn(ce_loss, num_errors_tensor[1:])\n","        else:\n","            raise Exception(\"Incorrect loss type.\")\n","        \n","        \n","        return total_loss"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2024-08-31T18:24:16.300115Z","iopub.status.busy":"2024-08-31T18:24:16.299717Z","iopub.status.idle":"2024-08-31T18:24:16.320068Z","shell.execute_reply":"2024-08-31T18:24:16.318979Z","shell.execute_reply.started":"2024-08-31T18:24:16.300085Z"},"trusted":true},"outputs":[],"source":["def train_model(model, train_loader, optimizer, loss_fn):\n","\n","    model.train()\n","\n","    train_loss = 0\n","    for batch in train_loader:\n","        batch = tuple(v.to(device) for t, v in batch.items())\n","        batch_input_ids, batch_token_type_ids, batch_attention_mask, batch_labels = batch\n","        \n","        loss, outputs = None, None\n","        \n","        logits = model(batch_input_ids, attention_mask = batch_attention_mask, token_type_ids = batch_token_type_ids, labels = batch_labels)\n","        \n","\n","        loss = loss_fn.compute_loss(logits, batch_labels)\n","\n","        train_loss += loss.item()\n","\n","        # backprop\n","        optimizer.zero_grad()\n","        \n","        loss.backward()\n","        \n","        # gradient clipping\n","        torch.nn.utils.clip_grad_norm_(parameters=model.parameters(), max_norm=1.0)\n","\n","        optimizer.step()\n","\n","    avg_train_loss = round((train_loss / len(train_loader)), 4)\n","    return avg_train_loss\n","\n","def evaluate_model(model, dataloader, tokenizer, loss_fn):\n","\n","    model.eval()\n","\n","    eval_loss = 0\n","    eval_tokens, eval_labels, eval_predictions = [], [], []\n","    \n","    with torch.no_grad():\n","        for batch in dataloader:\n","            batch = tuple(v.to(device) for t, v in batch.items())\n","            batch_input_ids, batch_token_type_ids, batch_attention_mask, batch_labels = batch\n","            logits = model(batch_input_ids, attention_mask = batch_attention_mask, token_type_ids = batch_token_type_ids, labels = batch_labels)\n","    \n","            loss = loss_fn.compute_loss(logits, batch_labels, validation = True)\n","            \n","            eval_loss += loss.item()\n","    \n","            batch_labels = batch_labels.detach().cpu().numpy()\n","            batch_predictions = np.argmax(logits.detach().cpu().numpy(), axis=2)\n","            \n","            clean_elements = clean_batch_elements(batch_input_ids, batch_labels, batch_predictions, tokenizer)\n","            \n","            eval_tokens += clean_elements[0]\n","            eval_labels += clean_elements[1]\n","            eval_predictions += clean_elements[2]\n","    \n","    flattened_labels = [j for sub in eval_labels for j in sub]\n","    flattened_predictions = [j for sub in eval_predictions for j in sub]\n","    \n","    eval_f1 = f1_score(flattened_labels, flattened_predictions, average = 'macro')\n","    return round((eval_loss / len(dataloader)), 4), eval_f1\n","\n","\n","def test_model(model, dataloader, tokenizer):\n","\n","    model.eval()\n","\n","    eval_tokens, eval_labels, eval_predictions = [], [], []\n","\n","    with torch.no_grad():\n","        for batch in dataloader:\n","            batch = tuple(v.to(device) for t, v in batch.items())\n","            batch_input_ids, batch_token_type_ids, batch_attention_mask, batch_labels = batch\n","\n","            logits = model(batch_input_ids, token_type_ids = batch_token_type_ids, attention_mask = batch_attention_mask, labels = batch_labels)\n","    \n","            batch_labels = batch_labels.detach().cpu().numpy()\n","            batch_predictions = np.argmax(logits.detach().cpu().numpy(), axis=2)\n","            \n","            clean_elements = clean_batch_elements(batch_input_ids, batch_labels, batch_predictions, tokenizer)\n","            \n","            eval_tokens += clean_elements[0]\n","            eval_labels += clean_elements[1]\n","            eval_predictions += clean_elements[2]\n","\n","    return eval_tokens, eval_labels, eval_predictions"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-08-31T18:24:22.793349Z","iopub.status.busy":"2024-08-31T18:24:22.792630Z"},"trusted":true},"outputs":[],"source":["# loading data\n","dataname = 'pe' # datasets cited in the manuscript.\n","df = pd.read_csv(f'/kaggle/input/error-aware-argument-identification/{dataname}_sent.csv')\n","print(df.shape)\n","\n","# cross-dataset testing\n","cross_tests = [x for x in ['pe', 'we', 'abstrct'] if x != dataname]\n","\n","best_model = None\n","best_loss = float('inf')\n","best_run = 0\n","\n","experiments_configuration = Config(runs = 10, epochs = 10, batch_size = 32, error_type = 'ce-sum-weighted-errors-dynamic', use_crf = False)\n","\n","for nrun in range(experiments_configuration.runs):\n","    rs = generate_random_seed()\n","    set_random_seed(rs)\n","    \n","    run_best_eval_loss = float('inf')\n","    run_best_epoch = 0\n","    run_best_model_state = None\n","    \n","    train_loader, val_loader, test_loader = load_data(df, experiments_configuration)\n","    \n","    # Create model\n","    tagger = SimpleTagger(experiments_configuration)\n","\n","    tagger.to(device)\n","    \n","    base_loss_fn = nn.CrossEntropyLoss()\n","    initial_category_weights = [1, 1, 1, 1, 1] # disp, sp, mg, mu, unr\n","    \n","    custom_loss = CustomLoss(base_loss_fn, initial_category_weights, experiments_configuration)\n","    \n","    optimizer = torch.optim.AdamW(tagger.parameters(), lr = 1e-4, eps = 1e-8)\n","    \n","    weights_info = []\n","    acummulated_errors_info = []\n","    train_eval_info = []\n","    \n","    # training and validation\n","    for epoch in range(experiments_configuration.epochs):\n","                \n","        train_loss = train_model(tagger, train_loader, optimizer, custom_loss)\n","        \n","        eval_loss, eval_f1 = evaluate_model(tagger, val_loader, experiments_configuration.tokenizer, custom_loss)\n","        \n","        print(f\"Train_loss={train_loss} | Eval_loss={eval_loss} | Eval_F1={eval_f1}\")\n","                        \n","        weights_info.append([epoch] + custom_loss.get_category_weights())\n","        acummulated_errors_info.append([epoch] + custom_loss.acumulated_error_epoch.tolist())\n","        train_eval_info.append([epoch, train_loss, eval_loss, eval_f1])\n","        \n","        if experiments_configuration.error_type == 'ce-sum-weighted-errors-dynamic':\n","            custom_loss.update_category_weights(epoch)\n","            \n","        custom_loss.restart_error_counter()        \n","        \n","        if eval_loss < run_best_eval_loss:\n","            run_best_eval_loss = eval_loss\n","            run_best_epoch = epoch\n","            run_best_model_state = copy.deepcopy(tagger.state_dict())\n","            print(f\"New best model in epoch {epoch} (eval_loss = {eval_loss})\")\n","            \n","    # testing\n","    best_tagger = SimpleTagger(experiments_configuration)\n","        \n","    best_tagger.load_state_dict(run_best_model_state)\n","    best_tagger.to(device)\n","    \n","    tokens, labels, preds = test_model(best_tagger, test_loader, experiments_configuration.tokenizer)\n","    assert len(tokens) == len(labels) == len(preds)\n","    if len(tokens) > 0:\n","        assert len(tokens[0]) == len(labels[0]) == len(preds[0])\n","        \n","    save_predictions(tokens, labels, preds, f'test_{nrun}_{dataname}.txt')\n","    \n","    ## cross-testing\n","    for test_dataname in cross_tests:\n","        cross_df = pd.read_csv(f'/kaggle/input/error-aware-argument-identification/{test_dataname}_sent.csv')\n","        _, _, cross_test_loader = load_data(cross_df, experiments_configuration)\n","        tokens, labels, preds = test_model(best_tagger, cross_test_loader, experiments_configuration.tokenizer)\n","        assert len(tokens) == len(labels) == len(preds)\n","        save_predictions(tokens, labels, preds, f'test_{nrun}_{dataname}_{test_dataname}.txt')\n","    \n","    if experiments_configuration.error_type == 'ce-sum-weighted-errors-dynamic':\n","        pd.DataFrame(weights_info, columns = ['epoch', 'disp', 'sp', 'mg', 'mu', 'unr']).to_csv(f'weights_{nrun}_{dataname}.csv', index = False)\n","    \n","    pd.DataFrame(acummulated_errors_info, columns = ['epoch', 'pm', 'disp', 'sp', 'mg', 'mu', 'unr']).to_csv(f'errors_{nrun}_{dataname}.csv', index = False)\n","    pd.DataFrame(train_eval_info, columns = ['epoch', 'train_loss', 'eval_loss', 'eval_f1']).to_csv(f'metrics_{nrun}_{dataname}.csv', index = False)\n","            \n","    if run_best_eval_loss < best_loss:\n","        best_loss = run_best_eval_loss\n","        best_model = copy.deepcopy(run_best_model_state)\n","        best_run = nrun\n","        \n","    print(f\"end run {nrun}\")\n","    print()\n","    \n","# saving best model\n","model_path = f\"model-{best_run}-{dataname}.pt\"\n","if best_model is not None:\n","    torch.save(best_model, model_path)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["import os\n","import zipfile\n","from IPython.display import FileLink\n","\n","def zip_files(folder_path, zip_name):\n","    # Crear un archivo ZIP\n","    with zipfile.ZipFile(zip_name, 'w', zipfile.ZIP_DEFLATED) as zipf:\n","        # Recorrer todos los archivos en la carpeta\n","        for foldername, subfolders, filenames in os.walk(folder_path):\n","            for filename in filenames:\n","                # Comprobar si el archivo es un archivo TXT o CSV\n","                if filename.endswith('.txt') or filename.endswith('.csv') or filename.endswith('.pt'):\n","                    # Ruta completa del archivo\n","                    file_path = os.path.join(foldername, filename)\n","                    # Agregar el archivo al archivo ZIP\n","                    zipf.write(file_path, os.path.relpath(file_path, folder_path))\n","\n","# Llamar a la funciÃ³n para comprimir los archivos\n","folder_path = '/kaggle/working/'\n","zip_name = 'test.zip'\n","zip_files(folder_path, zip_name)\n","\n","FileLink(r'test.zip')"]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"datasetId":4797681,"sourceId":9172810,"sourceType":"datasetVersion"}],"dockerImageVersionId":30746,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":4}
